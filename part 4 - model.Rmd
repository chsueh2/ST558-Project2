---
title: "ST558 Porject 1: Part 4"
author: "Bridget Knapp and Chien-Lan Hsueh"
institute: Online Statistics Master Program, NCSU
date: "`r Sys.Date()`"
output:
  html_notebook:
    theme: cerulean
    highlight: haddock
    code_folding: none
  html_document:
    df_print: paged
params:
  channel: "lifestyle"
---

```{r include=FALSE}
knitr::opts_chunk$set(fig.path = "./images/")
```


```{r, include=FALSE}
knitr::knit("part 2 - data.Rmd", output = tempfile())

# subset by channel
df_train_x <- df_train %>% filter(channel == params$channel)
df_test_x <- df_test %>% filter(channel == params$channel)
```


```{r}
# subset by channel (rmarkdown parameter)
df_train_x <- df_train %>% filter(channel == params$channel)
df_test_x <- df_test %>% filter(channel == params$channel)
```



## Modeling
In this project, we model the response `shares` using supervised learning including linear regression, random forests and boosted tree models. For each, one of the following subsets of predictors is used to model the response:

- Model A: `shares` ~ `weekday` (categorical) + numbers of various tokens, words, links, images and video (numeric)
- Model B: `shares` ~ `is_weekend` (categorical) + all numeric predictors (numeric)

```{r}
# Model A: `weekday` (categorical) + selected numeric predictors
vars_A <- c(1, 3, 5:13, 15)
names(df_train_x)[vars_A]

# Model B: `is_weekend` (categorical) + all numeric predictors
vars_B <- c(1, 4, 5:48)
names(df_train_x)[vars_B]
```

### Linear Regression Model

A linear regression models the relationship between a response and predictors with a linear predictor functions. The model parameters are estimated from the data by minimizing a loss function. One of the common loss function is root mean squared error (RMSE) which is the standard deviation of the prediction errors (residuals). 

We fit both model A and B using the training data and compare their performance on the training set using 5-fold cross-validation. The best model is then used to predict on the test set to evaluate the model performance.

```{r, warning=FALSE}
# linear regression models

# Model A: `weekday` (categorical) + selected numeric predictors
fit_lm_A <- fit_model(
  shares ~ ., df_train_x[, vars_A], df_test_x[, vars_A], method = "lm",
  trControl = trainControl(method = "cv", number = 5))

# Model B: `is_weekend` (categorical) + all numeric predictors
fit_lm_B <- fit_model(
  shares ~ ., df_train_x[, vars_B], df_test_x[, vars_B], method = "lm",
  trControl = trainControl(method = "cv", number = 5))
```

### Random Forests Model

Random forests is also known as random decision forests model. It is an ensemble method based on decision trees. It uses bagging to create multiple trees from bootstrap samples and aggregate the results to make decisions. However, instead of all predictors, only a subset of the predictors are used to grow trees. This effectively prevents highly correlated trees if there exists a strong predictor. 


In addition to `randomForest` package, we also use `ranger` package. Again, 5-fold cross validation is used to choose the best model.


```{r}
# random forest models

# Model A: `weekday` (categorical) + selected numeric predictors
fit_rf_A <- fit_model(
  shares ~ ., df_train_x[, vars_A], df_test_x[, vars_A], method = "rf",
  trControl = trainControl(method = "cv", number = 5))

# Model B: `is_weekend` (categorical) + all numeric predictors
fit_rf_B <- fit_model(
  shares ~ ., df_train_x[, vars_B], df_test_x[, vars_B], method = "rf",
  trControl = trainControl(method = "cv", number = 5))
```

```{r}
# random forests models (ranger)

# Model A: `weekday` (categorical) + selected numeric predictors
fit_ranger_A <- fit_model(
  shares ~ ., df_train_x[, vars_A], df_test_x[, vars_A], method = "ranger",
  trControl = trainControl(method = "cv", number = 5))

# Model B: `is_weekend` (categorical) + all numeric predictors
fit_ranger_B <- fit_model(
  shares ~ ., df_train_x[, vars_B], df_test_x[, vars_B], method = "ranger",
  trControl = trainControl(method = "cv", number = 5))
```

### Boosted Tree Model

Last, we try boosted tree models in which trees are grown sequentially. Each subsequent tree is grown on a modified version of the original tree and thus the prediction is updated as the tree grown.

We use 5-fold cross validation to determine the best parameters.


```{r}
# boosted tree models

# Model A: `weekday` (categorical) + selected numeric predictors
fit_boosted_A <- fit_model(
  shares ~ ., df_train_x[, vars_A], df_test_x[, vars_A], method = "gbm",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(
    n.trees = seq(5, 200, 5),
    interaction.depth = 1:4,
    shrinkage = 0.1,
    n.minobsinnode =10),
  verbose = FALSE)

# Model B: `is_weekend` (categorical) + all numeric predictors
fit_boosted_B <- fit_model(
  shares ~ ., df_train_x[, vars_B], df_test_x[, vars_B], method = "gbm",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(
    n.trees = seq(5, 200, 5),
    interaction.depth = 1:4,
    shrinkage = 0.1,
    n.minobsinnode =10),
  verbose = FALSE)
```

## Comparison

We use RMSE to compare the model performance on the test set:

```{r}
df_comparison <- tibble(
    Linear_Regression = c(fit_lm_A["RMSE"], fit_lm_B["RMSE"]),
    Random_Forests = c(fit_rf_A["RMSE"], fit_rf_B["RMSE"]),
    Random_Forests_ranger = c(fit_ranger_A["RMSE"], fit_ranger_B["RMSE"]),
    Boosted_Tree = c(fit_boosted_A["RMSE"], fit_boosted_B["RMSE"])
  ) %>% 
  bind_cols(model = c("A: shares ~ weekday + selected numeric vars", "B: shares ~ is_weekend + all numeric vars")) %>% 
  pivot_longer(
    cols = !model,
    names_to = "learning_method",
    values_to = "RMSE"
  ) %>% 
  arrange(RMSE)

df_comparison
```
For the news data in ``r params$channel`` category, we found that the model ``r df_comparison$model[1]`` using supervised learning ``r df_comparison$learning_method[1]`` has the lowest RMSE of `r df_comparison$RMSE[1]`.


