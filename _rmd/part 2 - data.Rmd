---
title: "ST558 Porject 1: Part 2"
author: "Bridget Knapp and Chien-Lan Hsueh"
institute: Online Statistics Master Program, NCSU
date: "`r Sys.Date()`"
output:
  html_notebook:
    theme: cerulean
    highlight: haddock
    code_folding: none
  html_document:
    df_print: paged
params:
  channel: "all"
  load_data: TRUE
---


## Set up

### Packages
We will use the following packages in this project:

- `here`: enables easy file referencing and builds file paths in a OS-independent way
- `stats`: loads this before loading `tidyverse` to avoid masking some `tidyverse` functions
- `tidyverse`: includes collections of useful packages like `dplyr` (data manipulation), `tidyr` (tidying data),  `ggplots` (creating graphs), etc.
- `lubridate`: handle date and datetime data type
- `glue`: offers interpreted string literals for easy creation of dynamic messages and labels
- `scales`: formats and labels scales nicely for better visualization
- `skimr`: summary statistics about variables in data frames
- `ggcorrplot`: correlation plot matrix
- `caret`: training and plotting classification and regression models
- `randomForest`: random forest algorithm for classification and regression.
- `ranger`: a fast implementation of random forests 
- `gbm`: generalized boosted regression models

In addition, the `pacman` package provides handy tools to manage R packages (install, update, load and unload). We use its `p_laod()` instead of `libarary()` to load the packages listed above. 

```{r, results='hide'}
if (!require("pacman")) utils::install.packages("pacman", dependencies = TRUE)

pacman::p_load(
  here,
  stats,
  tidyverse,
  lubridate,
  glue, scales, skimr,
  ggcorrplot,
  caret, randomForest, ranger, gbm
)
```

### Helper Functions

We define a helper function `fit_model()` wrapping `caret::train()`. This function trains models on training data set and test the model performance on test data set.

> Arguments:
>
> - `formula`: formula
> - `df_train`: training set
> - `df_test`: test set
> - `method`: classification or regression model to use
> - `preProcess`: pre-processing of the predictors
> - `trControl`: a list of values that define how train acts
> - `tuneGrid`: a data frame with possible tuning values
> - `plot`: whether to plot parameter and metric
> - `...`: arguments passed to the classification or regression routine
>
> Returned Value: a performance metric (for a numeric response) or confusion matrix (for a categorical response)

In case a tuning parameter plot is needed, this function calls a helper function `plot_modelinfo()` (defined below) to extract model information including the tuning parameter and the corresponded metric to produce a scatter plot.


```{r}
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
    formula, df_train, df_test, method, 
    preProcess = c("center", "scale"),
    trControl = trainControl(), 
    tuneGrid = NULL, 
    plot = TRUE, ... ){
  
  # timer - start
  proc_timer <- proc.time()
  
  # train model
  fit <- train(
    form = formula,
    data = df_train,
    method = method,
    preProcess = c("center", "scale"),
    trControl = trControl,
    tuneGrid = tuneGrid, ...)
  
  # timer - report time used
  print(proc.time() - proc_timer)
  
  # print the best tune if there is a tuning parameter
  if(is.null(tuneGrid)){
    print("No tuning parameter")
  } else {
    # print the best tune 
    print("The best tune is found with:")
    print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
    if(plot) plot_modelinfo(fit)
  }
  
  # make prediction on test set
  pred <- predict(fit, newdata = df_test)
  
  # return performance metric or confusion matrix depending on response type
  if(is.numeric(pred)){
    # numeric response
    performance <- postResample(pred, obs = df_test[, 1])
    # print performance metrics
    print("Performance metrics:")
    print(performance)
    
    # return the performance metric
    return(performance)
    
  } else if(is.factor(pred)){
    # categorical response
    cfm <- confusionMatrix(df_test[, 1], pred)
    # print confusion matrix and accuracy
    print("Confusion table:")
    print(cfm$table)
    print(glue("Accuracy = {cfm$overall['Accuracy']}"))
    
    # return the confusion matrix
    return(cfm)
  }
}

# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
  # get model info
  model <- fit$modelInfo$label
  parameter <- fit$modelInfo$parameters$parameter
  description <- fit$modelInfo$parameters$label
  
  # plot parameter vs metrics
  p <- fit$results %>% 
    rename_at(1, ~"x") %>% 
    pivot_longer(cols = -1, names_to = "Metric") %>% 
    ggplot(aes(x, value, color = Metric)) +
    geom_point() +
    geom_line() +
    facet_grid(rows = vars(Metric), scales = "free_y") +
    labs(
      title = glue("{model}: Hyperparameter Tuning"),
      x = glue("{parameter} ({description})")
    )
  print(p)
  return(p)
}
```


## Load and Prep Data

The data file is saved in the `data` folder. After reading the raw data, we first check what variables it contains and if there is any missing or NA data.

```{r}
df_raw <- read_csv(here("data", "OnlineNewsPopularity.csv"))
 
# show the raw data
head(df_raw)

# check structure
str(df_raw)

# check if any missing values
anyNA(df_raw)
```


First we check on the six `data_channel_is_*` columns with an one-way table on the sum of their values for each row (record):

```{r}
# create a count table on the sum of the six `data_channel_is_*` columns
tbl_channel_raw <- df_raw %>% 
  # deal with columns of interest only for computing efficiency
  select(starts_with("data_channel_is_")) %>% 
  mutate(sum_of_channels = rowSums(across(everything()))) %>% 
  select(sum_of_channels) %>% 
  # create an one-way table for counts
  table()

tbl_channel_raw
```


Among the `r sum(tbl_channel_raw)` record, `r tbl_channel_raw["1"]` records are from one of the 6 channels (`lifestyle`, `entertainment`, `bus` for business, `socmed` for social media, `tech` and `world`) and the other `r tbl_channel_raw["0"]` are from the other channel (not included in these 6 channels). Therefore, we can create a new data channel `misc` to label the. 


Next, we will prepare the data with the following steps:

- remove `url` and `timedelta` since we are not using them as predictors (non-predictive)
- create a new column `data_channel_is_misc` for those records not from the 6 channels
- convert the 7 `data_channel_is_*` columns (original 6 + newly created 1 in the last step) into a long-form column `channel`
- convert the 7 `weekday_is_*` columns into a long-column `weekday`
- remove the columns created in the intermediate steps
- convert categorical variables `channel`, `weekday` and `is_weekend` to factors
- move the response variable `shares` and the categorical predictors `channel`, `weekday` and `is_weekend` to the first columns for easy handling when analyzing models


```{r}
df <- df_raw %>% 
  select(-url, -timedelta) %>% 
  mutate(
    sum_of_channels = rowSums(across(starts_with("data_channel_is_"))),
    # label records not from the 6 channels
    data_channel_is_misc = as.integer(sum_of_channels == 0)
  ) %>% 
  # pivot the channel columns (to a long form)
  pivot_longer(
    cols = starts_with("data_channel_is_"),
    names_to = "channel",
    names_prefix = "data_channel_is_",
    values_to = "channel_indicator"
  ) %>% 
  # pivot the channel columns (to a long form)
  pivot_longer(
    cols = starts_with("weekday_is_"),
    names_to = "weekday",
    names_prefix = "weekday_is_",
    values_to = "weekday_indicator"
  ) %>% 
  # remove redundant records
  filter(
    channel_indicator == 1,
    weekday_indicator == 1
  ) %>% 
  # remove redundant columns
  select(-sum_of_channels, -channel_indicator, -weekday_indicator) %>% 
  # convert categorical variables to factors
  mutate(
    channel = factor(channel),
    weekday = factor(weekday, levels = levels(wday(Sys.Date(), label = T, abbr = F)) %>% str_to_lower()),
    is_weekend = if_else(is_weekend == 1, "Y", "N") %>% factor()
  ) %>% 
  relocate(shares, channel, weekday, is_weekend)

# check numer of rows
nrow(df_raw) == nrow(df)

# 2-way contingency table
table(df$channel, df$weekday)
```
Note that the data frame has the same number of rows with the raw data frame, and it has `r length(unique(df$channel))` channels.


## Split the Data

We use `caret::createDataPartition()` to create a 70/30 split of training and test sets. This is done with `set.seed()`
to make this analysis reproducible.
```{r}
set.seed(2022)

# split data
trainIndex <- createDataPartition(df$shares, p = 0.7, list = F)
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]
```


Check if both train and test sets have similar cdf:
```{r}
# check balance - compare ecdf of train and test sets
plot(ecdf(df_train$shares), col = "blue", main = "Empirical Cumulative Distribution", xlab = "shares")
plot(ecdf(df_test$shares), col = "red", add = T)
```






